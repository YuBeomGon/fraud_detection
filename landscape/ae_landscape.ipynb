{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9967f2f3-101e-427e-ab45-4f5c1978ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb and pytorch manual \n",
    "# https://colab.research.google.com/drive/1XDtq-KT0GkX06a_g1MevuLFOMk4TxjKZ#scrollTo=bZpt5W2NNl6S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be15c793-b481-4c47-a09d-db027f5462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "import logging\n",
    "logging.propagate = False\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa83412-5353-4dd9-9964-3f5043a65aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeomgon-yu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee16f75-0753-444f-a90e-09e738ed2a64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc0857c-d264-4a34-8e75-9e30a381b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeomgon-yu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beomgon/dacon/fraud_detection/wnb/wandb/run-20220719_092012-1atyen68</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgon-yu/Credit%20Card%20Fraud%20Detection/runs/1atyen68\" target=\"_blank\">20220719_092011</a></strong> to <a href=\"https://wandb.ai/beomgon-yu/Credit%20Card%20Fraud%20Detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "wandb.init(project='Credit Card Fraud Detection',  name=now, mode='online')\n",
    "wandb.watch_called = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config =wandb.config\n",
    "config.batch_size = 256\n",
    "config.epochs = 100\n",
    "config.lr = 5e-3\n",
    "config.momentum = 0.5\n",
    "config.weight_decay = 1e-4\n",
    "config.device = device\n",
    "config.seed = 42\n",
    "config.log_interval = 10\n",
    "config.num_workers = 8\n",
    "config.adam = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7ac05d-9b37-41cc-a1e1-5d190aeeff06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1d5ce-739a-4136-a323-0be4f0a1ed1a",
   "metadata": {},
   "source": [
    "# set seed for reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2331a56b-4170-467d-a587-ae661a716086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed) :\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcfa15-01fb-4176-af2d-d5f0640bc9c1",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2074f8c2-ddb0-453b-9065-cfe3bc4630b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is 18 times slower than Numpy (15.8ms vs 0.874 ms). Pandas is 20 times slower than Numpy\n",
    "# therefore Use numpy for faster data loading\n",
    "\n",
    "class CDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode=False):\n",
    "        self.df = df\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = self.df['Class'].values\n",
    "            self.df = self.df.drop(columns=['Class']).values\n",
    "        else:\n",
    "            self.df = self.df.values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97ad33d-0c21-4ea3-b21c-8921a5d0371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CDataset(train_df)\n",
    "# train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "\n",
    "# val_dataset = CDataset(val_df, eval_mode=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f87fac-80ba-408d-bcd5-daf8f705ed46",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ed1d6c-1f7a-4152-a4aa-21df8714f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        # self.act = nn.ReLU()\n",
    "        self.act = nn.GELU()\n",
    "        # self.act = nn.LeakyReLU()\n",
    "        \n",
    "        self.dim = 30\n",
    "        self.hidden1 = 64\n",
    "        self.hidden2 = 128\n",
    "        self.drop_rate = 0.\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.dim),\n",
    "            nn.Linear(self.dim,self.hidden1),\n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            self.act,\n",
    "            nn.Dropout(p=self.drop_rate),\n",
    "            nn.Linear(self.hidden1,self.hidden2),\n",
    "            nn.BatchNorm1d(self.hidden2),\n",
    "            self.act,\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.hidden2,self.hidden1),\n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            self.act,\n",
    "            nn.Dropout(p=self.drop_rate),\n",
    "            nn.Linear(self.hidden1,self.dim),\n",
    "        )\n",
    "    \n",
    "          # tied auto encoder\n",
    "#         self.l1_weight = torch.randn(self.hidden1, self.dim) / torch.sqrt(torch.tensor(self.hidden1))\n",
    "#         self.l1_bias = torch.zeros(self.hidden1)\n",
    "        \n",
    "#         self.l2_weight = torch.randn(self.hidden2, self.hidden1) / torch.sqrt(torch.tensor(self.hidden2))\n",
    "#         self.l2_bias = torch.zeros(self.hidden2)\n",
    "\n",
    "#         self.encoder[1].weight = nn.Parameter(self.l1_weight)\n",
    "#         self.encoder[1].bais = nn.Parameter(self.l1_bias)\n",
    "#         self.encoder[4].weight = nn.Parameter(self.l2_weight)\n",
    "#         self.encoder[4].bias = nn.Parameter(self.l2_bias)\n",
    "\n",
    "#         self.decoder[0].weight = nn.Parameter(self.l2_weight.transpose(0,1))\n",
    "#         self.encoder[0].bais = nn.Parameter(self.l2_bias)\n",
    "#         self.encoder[3].weight = nn.Parameter(self.l1_weight.transpose(0,1))\n",
    "#         self.encoder[3].bias = nn.Parameter(self.l1_bias)   \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)          \n",
    "        \n",
    "    def forward(self, x) :\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2bc72-e5a3-4abc-8226-7f73884b7ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5193792-9634-4435-bf49-eea18ff4afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer() :\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, config) :\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = config.device\n",
    "        self.epochs = config.epochs\n",
    "        self.lr = config.lr\n",
    "        \n",
    "        self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self,) :\n",
    "        self.model.to(self.device)\n",
    "        best_score = 0\n",
    "        for epoch in range(self.epochs) :\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            \n",
    "            for x in iter(self.train_loader) :\n",
    "                x = x.to(self.device)\n",
    "                _x = self.model(x)\n",
    "                \n",
    "                loss = self.criterion(x, _x)\n",
    "                loss = self.criterion(x, _x)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss.append(loss.item())\n",
    "                wandb.log({'train loss' : loss.item()})\n",
    "                \n",
    "            score = self.validation(self.model, 0.95)\n",
    "            # score = round(score, 3)\n",
    "            \n",
    "            if self.scheduler is not None :\n",
    "                self.scheduler.step(score)\n",
    "            \n",
    "            print(f'epoch :[{epoch}] train loss [{np.mean(train_loss):3f}] val score [{score:.3f}]')\n",
    "            \n",
    "            # for param_group in self.optimizer.param_groups:\n",
    "            #     print(param_group['lr'])      \n",
    "            \n",
    "            # print(f'epoch :[{epoch}] train loss [{np.mean(train_loss)}] val score [{score}] lr [{self.scheduler.get_lr()}]')\n",
    "\n",
    "            if best_score < score :\n",
    "                best_score = score\n",
    "                # torch.save(self.model.state_dict(), '../saved1/model_epoch-{}-f1-{:.3f}.pt'.format(epoch, best_score))\n",
    "                torch.save(self.model.state_dict(), '../saved1/best_model.pth')\n",
    "            \n",
    "    def validation(self, eval_model, threshold) :\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred_y = []\n",
    "        true_y = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader) :\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                _x = self.model(x)\n",
    "                diff = cos(x, _x).cpu().tolist()\n",
    "                batch_pred = np.where(np.array(diff) < threshold, 1, 0).tolist()\n",
    "                pred_y += batch_pred\n",
    "                true_y += y.tolist()\n",
    "                \n",
    "        f1 = f1_score(true_y, pred_y, average='macro')\n",
    "        wandb.log({'f1_score' : f1})\n",
    "                \n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cab059-b166-4f48-8467-788a39b3bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2311f239-f77c-4fe3-9c21-9b83d1503199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(config) :\n",
    "\n",
    "    train_df = pd.read_csv('../dataset/train.csv')\n",
    "    val_df = pd.read_csv('../dataset/val.csv')\n",
    "    train_df = train_df.drop(columns=['ID'])\n",
    "    val_df = val_df.drop(columns=['ID'])  \n",
    "    print(train_df.shape)\n",
    "    \n",
    "    train_dataset = CDataset(train_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "\n",
    "    val_dataset = CDataset(val_df, eval_mode=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = config.batch_size, shuffle=False, num_workers=config.num_workers)    \n",
    "    \n",
    "    seed_everything(config.seed)    \n",
    "\n",
    "    model = AutoEncoder()\n",
    "    model.eval()\n",
    "    \n",
    "    if config.adam :\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr = config.lr)\n",
    "    else :\n",
    "        optimizer = torch.optim.SGD(model.parameters(), config.lr,\n",
    "                                    momentum=config.momentum,\n",
    "                                    weight_decay=config.weight_decay)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "    # scheduler = StepLR(optimizer, step_size=50, gamma=0.2)\n",
    "    \n",
    "    wandb.watch(model, log='all')\n",
    "\n",
    "    trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, config)\n",
    "    \n",
    "    wandb.save('model.h5')\n",
    "\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b470b61-9168-497d-927a-937b32b39e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113842, 30)\n",
      "epoch :[0] train loss [0.226404] val score [0.558]\n",
      "epoch :[1] train loss [0.138719] val score [0.684]\n",
      "epoch :[2] train loss [0.124370] val score [0.781]\n",
      "epoch :[3] train loss [0.117217] val score [0.784]\n",
      "epoch :[4] train loss [0.112248] val score [0.805]\n",
      "epoch :[5] train loss [0.109178] val score [0.790]\n",
      "epoch :[6] train loss [0.106273] val score [0.833]\n",
      "epoch :[7] train loss [0.102976] val score [0.842]\n",
      "epoch :[8] train loss [0.100795] val score [0.842]\n",
      "epoch :[9] train loss [0.098284] val score [0.829]\n",
      "epoch :[10] train loss [0.097495] val score [0.842]\n",
      "epoch :[11] train loss [0.094733] val score [0.852]\n",
      "epoch :[12] train loss [0.094790] val score [0.852]\n",
      "epoch :[13] train loss [0.092553] val score [0.847]\n",
      "epoch :[14] train loss [0.092045] val score [0.857]\n",
      "epoch :[15] train loss [0.090396] val score [0.862]\n",
      "epoch :[16] train loss [0.089483] val score [0.879]\n",
      "epoch :[17] train loss [0.088306] val score [0.879]\n",
      "epoch :[18] train loss [0.087441] val score [0.842]\n",
      "epoch :[19] train loss [0.086727] val score [0.897]\n",
      "epoch :[20] train loss [0.086263] val score [0.842]\n",
      "epoch :[21] train loss [0.084723] val score [0.903]\n",
      "epoch :[22] train loss [0.084087] val score [0.839]\n",
      "epoch :[23] train loss [0.083926] val score [0.815]\n",
      "epoch :[24] train loss [0.083320] val score [0.583]\n",
      "epoch :[25] train loss [0.083199] val score [0.585]\n",
      "epoch :[26] train loss [0.081451] val score [0.500]\n",
      "epoch :[27] train loss [0.081063] val score [0.500]\n",
      "epoch :[28] train loss [0.080556] val score [0.500]\n",
      "epoch :[29] train loss [0.078881] val score [0.500]\n",
      "epoch :[30] train loss [0.078662] val score [0.500]\n",
      "epoch :[31] train loss [0.078025] val score [0.500]\n",
      "Epoch    33: reducing learning rate of group 0 to 2.5000e-03.\n",
      "epoch :[32] train loss [0.076949] val score [0.500]\n",
      "epoch :[33] train loss [0.072668] val score [0.500]\n",
      "epoch :[34] train loss [0.070972] val score [0.500]\n",
      "epoch :[35] train loss [0.072007] val score [0.500]\n",
      "epoch :[36] train loss [0.070906] val score [0.500]\n",
      "epoch :[37] train loss [0.071126] val score [0.500]\n",
      "epoch :[38] train loss [0.070389] val score [0.500]\n",
      "epoch :[39] train loss [0.070262] val score [0.500]\n",
      "epoch :[40] train loss [0.069675] val score [0.500]\n",
      "epoch :[41] train loss [0.069819] val score [0.500]\n",
      "epoch :[42] train loss [0.069402] val score [0.500]\n",
      "Epoch    44: reducing learning rate of group 0 to 1.2500e-03.\n",
      "epoch :[43] train loss [0.069369] val score [0.500]\n",
      "epoch :[44] train loss [0.066341] val score [0.500]\n",
      "epoch :[45] train loss [0.066692] val score [0.500]\n",
      "epoch :[46] train loss [0.066217] val score [0.500]\n",
      "epoch :[47] train loss [0.065778] val score [0.500]\n",
      "epoch :[48] train loss [0.066296] val score [0.500]\n",
      "epoch :[49] train loss [0.065988] val score [0.500]\n",
      "epoch :[50] train loss [0.064886] val score [0.500]\n",
      "epoch :[51] train loss [0.065052] val score [0.500]\n",
      "epoch :[52] train loss [0.065580] val score [0.500]\n",
      "epoch :[53] train loss [0.065058] val score [0.500]\n",
      "Epoch    55: reducing learning rate of group 0 to 6.2500e-04.\n",
      "epoch :[54] train loss [0.064710] val score [0.500]\n",
      "epoch :[55] train loss [0.064228] val score [0.500]\n",
      "epoch :[56] train loss [0.063081] val score [0.500]\n",
      "epoch :[57] train loss [0.063243] val score [0.500]\n",
      "epoch :[58] train loss [0.063477] val score [0.500]\n",
      "epoch :[59] train loss [0.063790] val score [0.500]\n",
      "epoch :[60] train loss [0.063131] val score [0.500]\n",
      "epoch :[61] train loss [0.063884] val score [0.500]\n",
      "epoch :[62] train loss [0.063154] val score [0.500]\n",
      "epoch :[63] train loss [0.063325] val score [0.500]\n",
      "epoch :[64] train loss [0.063277] val score [0.500]\n",
      "Epoch    66: reducing learning rate of group 0 to 3.1250e-04.\n",
      "epoch :[65] train loss [0.062873] val score [0.500]\n",
      "epoch :[66] train loss [0.062415] val score [0.500]\n",
      "epoch :[67] train loss [0.061938] val score [0.500]\n",
      "epoch :[68] train loss [0.061957] val score [0.500]\n",
      "epoch :[69] train loss [0.061574] val score [0.500]\n",
      "epoch :[70] train loss [0.062522] val score [0.500]\n",
      "epoch :[71] train loss [0.062504] val score [0.500]\n",
      "epoch :[72] train loss [0.061945] val score [0.500]\n",
      "epoch :[73] train loss [0.061908] val score [0.500]\n",
      "epoch :[74] train loss [0.061596] val score [0.500]\n",
      "epoch :[75] train loss [0.061379] val score [0.500]\n",
      "Epoch    77: reducing learning rate of group 0 to 1.5625e-04.\n",
      "epoch :[76] train loss [0.061979] val score [0.500]\n",
      "epoch :[77] train loss [0.061576] val score [0.500]\n",
      "epoch :[78] train loss [0.060972] val score [0.500]\n",
      "epoch :[79] train loss [0.061215] val score [0.500]\n",
      "epoch :[80] train loss [0.061673] val score [0.500]\n",
      "epoch :[81] train loss [0.060993] val score [0.500]\n",
      "epoch :[82] train loss [0.060846] val score [0.500]\n",
      "epoch :[83] train loss [0.061957] val score [0.500]\n",
      "epoch :[84] train loss [0.061448] val score [0.500]\n",
      "epoch :[85] train loss [0.061488] val score [0.500]\n",
      "epoch :[86] train loss [0.061388] val score [0.500]\n",
      "Epoch    88: reducing learning rate of group 0 to 7.8125e-05.\n",
      "epoch :[87] train loss [0.061113] val score [0.500]\n",
      "epoch :[88] train loss [0.060836] val score [0.500]\n",
      "epoch :[89] train loss [0.060993] val score [0.500]\n",
      "epoch :[90] train loss [0.061393] val score [0.500]\n",
      "epoch :[91] train loss [0.061030] val score [0.500]\n",
      "epoch :[92] train loss [0.061003] val score [0.500]\n",
      "epoch :[93] train loss [0.060914] val score [0.500]\n",
      "epoch :[94] train loss [0.060757] val score [0.500]\n",
      "epoch :[95] train loss [0.060463] val score [0.500]\n",
      "epoch :[96] train loss [0.060698] val score [0.500]\n",
      "epoch :[97] train loss [0.060891] val score [0.500]\n",
      "Epoch    99: reducing learning rate of group 0 to 3.9063e-05.\n",
      "epoch :[98] train loss [0.060349] val score [0.500]\n",
      "epoch :[99] train loss [0.060750] val score [0.500]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3d82f-01b7-499a-bb72-5ec435914623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71105fa-3474-428e-b0fb-e96336a2a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca0fff-9307-4b9f-b5dc-b225fc389300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce202d-839e-4339-bf7a-ccf24453f54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
