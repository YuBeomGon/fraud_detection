{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9967f2f3-101e-427e-ab45-4f5c1978ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb and pytorch manual \n",
    "# https://colab.research.google.com/drive/1XDtq-KT0GkX06a_g1MevuLFOMk4TxjKZ#scrollTo=bZpt5W2NNl6S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be15c793-b481-4c47-a09d-db027f5462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "import logging\n",
    "logging.propagate = False\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa83412-5353-4dd9-9964-3f5043a65aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeomgon-yu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee16f75-0753-444f-a90e-09e738ed2a64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc0857c-d264-4a34-8e75-9e30a381b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeomgon-yu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beomgon/dacon/abnormal_detection/wnb/wandb/run-20220718_180809-1f5pl9mi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgon-yu/Credit%20Card%20Fraud%20Detection/runs/1f5pl9mi\" target=\"_blank\">20220718_180808</a></strong> to <a href=\"https://wandb.ai/beomgon-yu/Credit%20Card%20Fraud%20Detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "wandb.init(project='Credit Card Fraud Detection',  name=now, mode='online')\n",
    "wandb.watch_called = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config =wandb.config\n",
    "config.batch_size = 256\n",
    "config.epochs = 100\n",
    "config.lr = 1e-2\n",
    "config.momentum = 0.1\n",
    "config.weight_decay = 1e-4\n",
    "config.device = device\n",
    "config.seed = 42\n",
    "config.log_interval = 10\n",
    "config.num_workers = 8\n",
    "config.adam = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7ac05d-9b37-41cc-a1e1-5d190aeeff06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1d5ce-739a-4136-a323-0be4f0a1ed1a",
   "metadata": {},
   "source": [
    "# set seed for reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2331a56b-4170-467d-a587-ae661a716086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed) :\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcfa15-01fb-4176-af2d-d5f0640bc9c1",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2074f8c2-ddb0-453b-9065-cfe3bc4630b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is 18 times slower than Numpy (15.8ms vs 0.874 ms). Pandas is 20 times slower than Numpy\n",
    "# therefore Use numpy for faster data loading\n",
    "\n",
    "class CDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode=False):\n",
    "        self.df = df\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = self.df['Class'].values\n",
    "            self.df = self.df.drop(columns=['Class']).values\n",
    "        else:\n",
    "            self.df = self.df.values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97ad33d-0c21-4ea3-b21c-8921a5d0371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CDataset(train_df)\n",
    "# train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "\n",
    "# val_dataset = CDataset(val_df, eval_mode=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f87fac-80ba-408d-bcd5-daf8f705ed46",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ed1d6c-1f7a-4152-a4aa-21df8714f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        # self.act = nn.ReLU()\n",
    "        self.act = nn.GELU()\n",
    "        # self.act = nn.LeakyReLU()\n",
    "        \n",
    "        self.dim = 30\n",
    "        self.hidden1 = 64\n",
    "        self.hidden2 = 128\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.dim),\n",
    "            nn.Linear(self.dim,self.hidden1),\n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            self.act,\n",
    "            nn.Linear(self.hidden1,self.hidden2),\n",
    "            nn.BatchNorm1d(self.hidden2),\n",
    "            self.act,\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.hidden2,self.hidden1),\n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            self.act,\n",
    "            nn.Linear(self.hidden1,self.dim),\n",
    "        )\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.dim),\n",
    "            nn.Linear(self.dim,self.hidden1),\n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            self.act,\n",
    "            nn.Linear(self.hidden1,self.hidden2),\n",
    "            nn.BatchNorm1d(self.hidden2),\n",
    "            self.act,\n",
    "        )\n",
    "        \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Linear(self.hidden2,self.hidden1),\n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            self.act,\n",
    "            nn.Linear(self.hidden1,self.dim),\n",
    "        )        \n",
    "    \n",
    "\n",
    "          # tied auto encoder\n",
    "#         self.l1_weight = torch.randn(self.hidden1, self.dim) / torch.sqrt(torch.tensor(self.hidden1))\n",
    "#         self.l1_bias = torch.zeros(self.hidden1)\n",
    "        \n",
    "#         self.l2_weight = torch.randn(self.hidden2, self.hidden1) / torch.sqrt(torch.tensor(self.hidden2))\n",
    "#         self.l2_bias = torch.zeros(self.hidden2)\n",
    "\n",
    "#         self.encoder[1].weight = nn.Parameter(self.l1_weight)\n",
    "#         self.encoder[1].bais = nn.Parameter(self.l1_bias)\n",
    "#         self.encoder[4].weight = nn.Parameter(self.l2_weight)\n",
    "#         self.encoder[4].bias = nn.Parameter(self.l2_bias)\n",
    "\n",
    "        \n",
    "#         self.decoder[0].weight = nn.Parameter(self.l2_weight.transpose(0,1))\n",
    "#         self.encoder[0].bais = nn.Parameter(self.l2_bias)\n",
    "#         self.encoder[3].weight = nn.Parameter(self.l1_weight.transpose(0,1))\n",
    "#         self.encoder[3].bias = nn.Parameter(self.l1_bias)   \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)          \n",
    "        \n",
    "    def forward(self, x) :\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x1 = self.decoder(x)\n",
    "        \n",
    "        x = self.encoder(x1)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x1, x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2bc72-e5a3-4abc-8226-7f73884b7ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5193792-9634-4435-bf49-eea18ff4afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer() :\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, config) :\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = config.device\n",
    "        self.epochs = config.epochs\n",
    "        self.lr = config.lr\n",
    "        \n",
    "        self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self,) :\n",
    "        self.model.to(self.device)\n",
    "        best_score = 0\n",
    "        for epoch in range(self.epochs) :\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            \n",
    "            for x in iter(self.train_loader) :\n",
    "                x = x.to(self.device)\n",
    "                x1, _x = self.model(x)\n",
    "                \n",
    "                loss = self.criterion(x, _x)\n",
    "                loss = self.criterion(x, _x) + 0.5*self.criterion(x, x1)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss.append(loss.item())\n",
    "                wandb.log({'train loss' : loss.item()})\n",
    "                \n",
    "            score = self.validation(self.model, 0.95)\n",
    "            \n",
    "            if self.scheduler is not None :\n",
    "                self.scheduler.step(score)\n",
    "            \n",
    "            print(f'epoch :[{epoch}] train loss [{np.mean(train_loss)}] val score [{score}]')\n",
    "            # for param_group in self.optimizer.param_groups:\n",
    "            #     print(param_group['lr'])      \n",
    "            \n",
    "            # print(f'epoch :[{epoch}] train loss [{np.mean(train_loss)}] val score [{score}] lr [{self.scheduler.get_lr()}]')\n",
    "\n",
    "            if best_score < score :\n",
    "                best_score = score\n",
    "                torch.save(self.model.state_dict(), '../saved/best_model.pth')\n",
    "            \n",
    "    def validation(self, eval_model, threshold) :\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred_y = []\n",
    "        true_y = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader) :\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                _, _x = self.model(x)\n",
    "                diff = cos(x, _x).cpu().tolist()\n",
    "                batch_pred = np.where(np.array(diff) < threshold, 1, 0).tolist()\n",
    "                pred_y += batch_pred\n",
    "                true_y += y.tolist()\n",
    "                \n",
    "        f1 = f1_score(true_y, pred_y, average='macro')\n",
    "        wandb.log({'f1_score' : f1})\n",
    "                \n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cab059-b166-4f48-8467-788a39b3bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2311f239-f77c-4fe3-9c21-9b83d1503199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(config) :\n",
    "\n",
    "    train_df = pd.read_csv('../dataset/train.csv')\n",
    "    val_df = pd.read_csv('../dataset/val.csv')\n",
    "    train_df = train_df.drop(columns=['ID'])\n",
    "    val_df = val_df.drop(columns=['ID'])  \n",
    "    print(train_df.shape)\n",
    "    \n",
    "    train_dataset = CDataset(train_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "\n",
    "    val_dataset = CDataset(val_df, eval_mode=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = config.batch_size, shuffle=False, num_workers=config.num_workers)    \n",
    "    \n",
    "    seed_everything(config.seed)    \n",
    "\n",
    "    model = AutoEncoder()\n",
    "    model.eval()\n",
    "    \n",
    "    if config.adam :\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr = config.lr)\n",
    "    else :\n",
    "        optimizer = torch.optim.SGD(model.parameters(), config.lr,\n",
    "                                    momentum=config.momentum,\n",
    "                                    weight_decay=config.weight_decay)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "    # scheduler = StepLR(optimizer, step_size=50, gamma=0.2)\n",
    "    \n",
    "    wandb.watch(model, log='all')\n",
    "\n",
    "    trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, config)\n",
    "    \n",
    "    wandb.save('model.h5')\n",
    "\n",
    "\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b470b61-9168-497d-927a-937b32b39e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113842, 30)\n",
      "epoch :[0] train loss [0.40344571193282525] val score [0.4900811722183891]\n",
      "epoch :[1] train loss [0.24218838666931966] val score [0.5032817761373315]\n",
      "epoch :[2] train loss [0.21708972176139274] val score [0.5046651501082147]\n",
      "epoch :[3] train loss [0.20629896156573563] val score [0.5070757950116161]\n",
      "epoch :[4] train loss [0.19740314808454407] val score [0.5090974190861624]\n",
      "epoch :[5] train loss [0.1919158743003781] val score [0.510969637786937]\n",
      "epoch :[6] train loss [0.18862779763307463] val score [0.511433290776233]\n",
      "epoch :[7] train loss [0.18013745583175272] val score [0.5267313678808235]\n",
      "epoch :[8] train loss [0.176950240938851] val score [0.5254002744367214]\n",
      "epoch :[9] train loss [0.1739639648895585] val score [0.5362409406460878]\n",
      "epoch :[10] train loss [0.17052484169769822] val score [0.541668475427458]\n",
      "epoch :[11] train loss [0.16739430054185095] val score [0.5507171941799006]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3d82f-01b7-499a-bb72-5ec435914623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445e390-75b7-41ab-82f9-57f82c642832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71105fa-3474-428e-b0fb-e96336a2a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca0fff-9307-4b9f-b5dc-b225fc389300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce202d-839e-4339-bf7a-ccf24453f54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
