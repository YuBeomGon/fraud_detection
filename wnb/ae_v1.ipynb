{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9967f2f3-101e-427e-ab45-4f5c1978ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb and pytorch manual \n",
    "# https://colab.research.google.com/drive/1XDtq-KT0GkX06a_g1MevuLFOMk4TxjKZ#scrollTo=bZpt5W2NNl6S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be15c793-b481-4c47-a09d-db027f5462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "import logging\n",
    "logging.propagate = False\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa83412-5353-4dd9-9964-3f5043a65aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeomgon-yu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee16f75-0753-444f-a90e-09e738ed2a64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc0857c-d264-4a34-8e75-9e30a381b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeomgon-yu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/beomgon/dacon/fraud_detection/wnb/wandb/run-20220723_195907-20z4tnsg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgon-yu/Credit%20Card%20Fraud%20Detection/runs/20z4tnsg\" target=\"_blank\">20220723_195907</a></strong> to <a href=\"https://wandb.ai/beomgon-yu/Credit%20Card%20Fraud%20Detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "wandb.init(project='Credit Card Fraud Detection',  name=now, mode='online')\n",
    "wandb.watch_called = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config =wandb.config\n",
    "config.batch_size = 512\n",
    "config.epochs = 100\n",
    "config.lr = 2e-3\n",
    "config.momentum = 0.5\n",
    "config.weight_decay = 1e-4\n",
    "config.device = device\n",
    "config.seed = 42\n",
    "config.log_interval = 10\n",
    "config.num_workers = 8\n",
    "config.adam = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7ac05d-9b37-41cc-a1e1-5d190aeeff06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1d5ce-739a-4136-a323-0be4f0a1ed1a",
   "metadata": {},
   "source": [
    "# set seed for reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2331a56b-4170-467d-a587-ae661a716086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed) :\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcfa15-01fb-4176-af2d-d5f0640bc9c1",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2074f8c2-ddb0-453b-9065-cfe3bc4630b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is 18 times slower than Numpy (15.8ms vs 0.874 ms). Pandas is 20 times slower than Numpy\n",
    "# therefore Use numpy for faster data loading\n",
    "\n",
    "class CDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode=False):\n",
    "        self.df = df\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = self.df['Class'].values\n",
    "            self.df = self.df.drop(columns=['Class']).values\n",
    "        else:\n",
    "            self.df = self.df.values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97ad33d-0c21-4ea3-b21c-8921a5d0371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CDataset(train_df)\n",
    "# train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "\n",
    "# val_dataset = CDataset(val_df, eval_mode=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f87fac-80ba-408d-bcd5-daf8f705ed46",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ed1d6c-1f7a-4152-a4aa-21df8714f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        # self.act = nn.ReLU()\n",
    "        self.act = nn.GELU()\n",
    "        # self.act = nn.LeakyReLU()\n",
    "        \n",
    "        self.dim = 30\n",
    "        self.hidden1 = 64\n",
    "        self.hidden2 = 128\n",
    "        self.drop_rate = 0.\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.dim),\n",
    "            nn.Linear(self.dim,self.hidden1),\n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            self.act,\n",
    "            # nn.Dropout(p=self.drop_rate),\n",
    "            nn.Linear(self.hidden1,self.hidden2),\n",
    "            nn.BatchNorm1d(self.hidden2),\n",
    "            self.act,\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.hidden2,self.hidden1),\n",
    "            nn.BatchNorm1d(self.hidden1),\n",
    "            self.act,\n",
    "            # nn.Dropout(p=self.drop_rate),\n",
    "            nn.Linear(self.hidden1,self.dim),\n",
    "        ) \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)          \n",
    "        \n",
    "    def forward(self, x) :\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2bc72-e5a3-4abc-8226-7f73884b7ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5193792-9634-4435-bf49-eea18ff4afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer() :\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, config) :\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = config.device\n",
    "        self.epochs = config.epochs\n",
    "        self.lr = config.lr\n",
    "        \n",
    "        self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self,) :\n",
    "        self.model.to(self.device)\n",
    "        best_score = 0\n",
    "        for epoch in range(self.epochs) :\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            \n",
    "            for x in iter(self.train_loader) :\n",
    "                x = x.to(self.device)\n",
    "                _x = self.model(x)\n",
    "                \n",
    "                loss = self.criterion(x, _x)\n",
    "                loss = self.criterion(x, _x)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss.append(loss.item())\n",
    "                wandb.log({'train loss' : loss.item()})\n",
    "                \n",
    "            score = self.validation(self.model, 0.95)\n",
    "            # score = round(score, 3)\n",
    "            \n",
    "            if self.scheduler is not None :\n",
    "                self.scheduler.step(score)\n",
    "            \n",
    "            print(f'epoch :[{epoch}] train loss [{np.mean(train_loss):3f}] val score [{score:.3f}]')\n",
    "\n",
    "            if best_score < score :\n",
    "                best_score = score\n",
    "                # torch.save(self.model.state_dict(), '../saved1/model_epoch-{}-f1-{:.3f}.pt'.format(epoch, best_score))\n",
    "                torch.save(self.model.state_dict(), '../saved1/best_model.pth')\n",
    "            \n",
    "    def validation(self, eval_model, threshold) :\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred_y = []\n",
    "        true_y = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader) :\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                _x = self.model(x)\n",
    "                diff = cos(x, _x).cpu().tolist()\n",
    "                batch_pred = np.where(np.array(diff) < threshold, 1, 0).tolist()\n",
    "                pred_y += batch_pred\n",
    "                true_y += y.tolist()\n",
    "                \n",
    "        f1 = f1_score(true_y, pred_y, average='macro')\n",
    "        wandb.log({'f1_score' : f1})\n",
    "                \n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cab059-b166-4f48-8467-788a39b3bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2311f239-f77c-4fe3-9c21-9b83d1503199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(config) :\n",
    "\n",
    "    train_df = pd.read_csv('../dataset/train.csv')\n",
    "    val_df = pd.read_csv('../dataset/val.csv')\n",
    "    train_df = train_df.drop(columns=['ID'])\n",
    "    val_df = val_df.drop(columns=['ID'])  \n",
    "    print(train_df.shape)\n",
    "    \n",
    "    train_dataset = CDataset(train_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "\n",
    "    val_dataset = CDataset(val_df, eval_mode=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = config.batch_size, shuffle=False, num_workers=config.num_workers)    \n",
    "    \n",
    "    seed_everything(config.seed)    \n",
    "\n",
    "    model = AutoEncoder()\n",
    "    model.eval()\n",
    "    \n",
    "    if config.adam :\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr = config.lr)\n",
    "    else :\n",
    "        optimizer = torch.optim.SGD(model.parameters(), config.lr,\n",
    "                                    momentum=config.momentum,\n",
    "                                    weight_decay=config.weight_decay)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "    # scheduler = StepLR(optimizer, step_size=50, gamma=0.2)\n",
    "    \n",
    "    wandb.watch(model, log='all')\n",
    "\n",
    "    trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, config)\n",
    "    \n",
    "    wandb.save('model.h5')\n",
    "\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b470b61-9168-497d-927a-937b32b39e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113842, 30)\n",
      "epoch :[0] train loss [0.346589] val score [0.477]\n",
      "epoch :[1] train loss [0.176636] val score [0.517]\n",
      "epoch :[2] train loss [0.143229] val score [0.565]\n",
      "epoch :[3] train loss [0.128652] val score [0.626]\n",
      "epoch :[4] train loss [0.118890] val score [0.695]\n",
      "epoch :[5] train loss [0.113446] val score [0.711]\n",
      "epoch :[6] train loss [0.108301] val score [0.757]\n",
      "epoch :[7] train loss [0.104257] val score [0.752]\n",
      "epoch :[8] train loss [0.101816] val score [0.757]\n",
      "epoch :[9] train loss [0.098686] val score [0.787]\n",
      "epoch :[10] train loss [0.097815] val score [0.790]\n",
      "epoch :[11] train loss [0.094930] val score [0.794]\n",
      "epoch :[12] train loss [0.095499] val score [0.781]\n",
      "epoch :[13] train loss [0.092894] val score [0.808]\n",
      "epoch :[14] train loss [0.092314] val score [0.820]\n",
      "epoch :[15] train loss [0.089998] val score [0.829]\n",
      "epoch :[16] train loss [0.090013] val score [0.838]\n",
      "epoch :[17] train loss [0.088630] val score [0.833]\n",
      "epoch :[18] train loss [0.088001] val score [0.842]\n",
      "epoch :[19] train loss [0.086285] val score [0.857]\n",
      "epoch :[20] train loss [0.086200] val score [0.824]\n",
      "epoch :[21] train loss [0.085138] val score [0.857]\n",
      "epoch :[22] train loss [0.084959] val score [0.862]\n",
      "epoch :[23] train loss [0.084628] val score [0.873]\n",
      "epoch :[24] train loss [0.083726] val score [0.862]\n",
      "epoch :[25] train loss [0.083943] val score [0.891]\n",
      "epoch :[26] train loss [0.082941] val score [0.867]\n",
      "epoch :[27] train loss [0.081713] val score [0.903]\n",
      "epoch :[28] train loss [0.081691] val score [0.903]\n",
      "epoch :[29] train loss [0.080459] val score [0.891]\n",
      "epoch :[30] train loss [0.080085] val score [0.903]\n",
      "epoch :[31] train loss [0.079872] val score [0.903]\n",
      "epoch :[32] train loss [0.079591] val score [0.903]\n",
      "epoch :[33] train loss [0.079805] val score [0.903]\n",
      "epoch :[34] train loss [0.078676] val score [0.903]\n",
      "epoch :[35] train loss [0.079217] val score [0.903]\n",
      "epoch :[36] train loss [0.077325] val score [0.903]\n",
      "epoch :[37] train loss [0.078750] val score [0.903]\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-03.\n",
      "epoch :[38] train loss [0.076947] val score [0.903]\n",
      "epoch :[39] train loss [0.073057] val score [0.903]\n",
      "epoch :[40] train loss [0.073107] val score [0.903]\n",
      "epoch :[41] train loss [0.072990] val score [0.903]\n",
      "epoch :[42] train loss [0.072696] val score [0.910]\n",
      "epoch :[43] train loss [0.073261] val score [0.910]\n",
      "epoch :[44] train loss [0.072440] val score [0.910]\n",
      "epoch :[45] train loss [0.072378] val score [0.910]\n",
      "epoch :[46] train loss [0.072368] val score [0.903]\n",
      "epoch :[47] train loss [0.071834] val score [0.910]\n",
      "epoch :[48] train loss [0.072073] val score [0.917]\n",
      "epoch :[49] train loss [0.073266] val score [0.910]\n",
      "epoch :[50] train loss [0.071233] val score [0.910]\n",
      "epoch :[51] train loss [0.071600] val score [0.910]\n",
      "epoch :[52] train loss [0.071796] val score [0.903]\n",
      "epoch :[53] train loss [0.070919] val score [0.910]\n",
      "epoch :[54] train loss [0.070444] val score [0.910]\n",
      "epoch :[55] train loss [0.071469] val score [0.917]\n",
      "epoch :[56] train loss [0.070982] val score [0.917]\n",
      "epoch :[57] train loss [0.070720] val score [0.910]\n",
      "epoch :[58] train loss [0.070962] val score [0.910]\n",
      "Epoch 00060: reducing learning rate of group 0 to 5.0000e-04.\n",
      "epoch :[59] train loss [0.070306] val score [0.917]\n",
      "epoch :[60] train loss [0.068450] val score [0.917]\n",
      "epoch :[61] train loss [0.069226] val score [0.917]\n",
      "epoch :[62] train loss [0.068131] val score [0.917]\n",
      "epoch :[63] train loss [0.067564] val score [0.910]\n",
      "epoch :[64] train loss [0.067908] val score [0.917]\n",
      "epoch :[65] train loss [0.067729] val score [0.917]\n",
      "epoch :[66] train loss [0.067654] val score [0.917]\n",
      "epoch :[67] train loss [0.067496] val score [0.917]\n",
      "epoch :[68] train loss [0.068647] val score [0.917]\n",
      "epoch :[69] train loss [0.067331] val score [0.917]\n",
      "Epoch 00071: reducing learning rate of group 0 to 2.5000e-04.\n",
      "epoch :[70] train loss [0.067944] val score [0.917]\n",
      "epoch :[71] train loss [0.066900] val score [0.917]\n",
      "epoch :[72] train loss [0.065844] val score [0.917]\n",
      "epoch :[73] train loss [0.066924] val score [0.917]\n",
      "epoch :[74] train loss [0.065959] val score [0.917]\n",
      "epoch :[75] train loss [0.065980] val score [0.917]\n",
      "epoch :[76] train loss [0.066776] val score [0.833]\n",
      "epoch :[77] train loss [0.066104] val score [0.917]\n",
      "epoch :[78] train loss [0.065462] val score [0.917]\n",
      "epoch :[79] train loss [0.065866] val score [0.917]\n",
      "epoch :[80] train loss [0.065934] val score [0.917]\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.2500e-04.\n",
      "epoch :[81] train loss [0.066115] val score [0.917]\n",
      "epoch :[82] train loss [0.065602] val score [0.917]\n",
      "epoch :[83] train loss [0.065045] val score [0.917]\n",
      "epoch :[84] train loss [0.065572] val score [0.910]\n",
      "epoch :[85] train loss [0.065576] val score [0.897]\n",
      "epoch :[86] train loss [0.065999] val score [0.917]\n",
      "epoch :[87] train loss [0.065272] val score [0.917]\n",
      "epoch :[88] train loss [0.065187] val score [0.917]\n",
      "epoch :[89] train loss [0.065165] val score [0.917]\n",
      "epoch :[90] train loss [0.064943] val score [0.917]\n",
      "epoch :[91] train loss [0.065278] val score [0.917]\n",
      "Epoch 00093: reducing learning rate of group 0 to 6.2500e-05.\n",
      "epoch :[92] train loss [0.065444] val score [0.917]\n",
      "epoch :[93] train loss [0.065028] val score [0.917]\n",
      "epoch :[94] train loss [0.064791] val score [0.917]\n",
      "epoch :[95] train loss [0.065052] val score [0.917]\n",
      "epoch :[96] train loss [0.064562] val score [0.917]\n",
      "epoch :[97] train loss [0.065096] val score [0.917]\n",
      "epoch :[98] train loss [0.064160] val score [0.917]\n",
      "epoch :[99] train loss [0.064295] val score [0.917]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71105fa-3474-428e-b0fb-e96336a2a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413946c-10c3-4867-853a-ad34e752ec43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
